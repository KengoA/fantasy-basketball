{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-01T13:44:04.014284Z",
     "start_time": "2018-04-01T13:44:03.293233Z"
    },
    "scrolled": false
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This notebook first generates standardized names from fantasy salary information by querying Basketball-Reference.com, and then merge two datasets while adding fantasy stats based on DraftKings rules. Position information is also modified and added from salary datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:05.924554Z",
     "start_time": "2019-05-26T09:32:05.420122Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from utils\n",
    "from constants import DATA_DIR, SECONDS_SLEEP, DF_VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:07.439131Z",
     "start_time": "2019-05-26T09:32:07.435317Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Fantasy Stats from Boxscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:45.478854Z",
     "start_time": "2019-05-26T09:32:45.465708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add additional columns for double double (DD) and triple double (TD)\n",
    "# Double digit in two or three of points, rebounds, assists, steals and blocks\n",
    "def add_doubles(df):\n",
    "    dd = [0 for i in range(df.shape[0])]\n",
    "    td = [0 for i in range(df.shape[0])]\n",
    "\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        doubles_count = 0\n",
    "        check_doubles = [\"PTS\", \"TRB\", \"AST\", \"STL\", \"BLK\"]\n",
    "\n",
    "        for stat in check_doubles:\n",
    "            if df.loc[i, stat] >= 10:\n",
    "                doubles_count += 1\n",
    "\n",
    "        if doubles_count >= 2:\n",
    "            dd[i] = 1\n",
    "        if doubles_count >= 3:\n",
    "            td[i] = 1\n",
    "\n",
    "    df[\"DD\"] = dd\n",
    "    df[\"TD\"] = td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Position Information on Salary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:47.683228Z",
     "start_time": "2019-05-26T09:32:47.676459Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_name_pos(df):\n",
    "    name_pos = {}\n",
    "\n",
    "    for name in set(df[\"Name\"]):\n",
    "        pos = df.loc[(df[\"Name\"] == name) & (df[\"Pos\"] != 0), \"Pos\"].mode()\n",
    "        if len(pos) != 0:\n",
    "            name_pos[name] = pos[0]\n",
    "\n",
    "    return name_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:50.268617Z",
     "start_time": "2019-05-26T09:32:50.258772Z"
    }
   },
   "outputs": [],
   "source": [
    "def fill_positions(df):\n",
    "    name_pos = generate_name_pos(df)\n",
    "\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        if df.loc[i, \"Pos\"] == 0:\n",
    "            name = df.loc[i, \"Name\"]\n",
    "            if name in name_pos.keys():\n",
    "                df.loc[i, \"Pos\"] = name_pos[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:51.176877Z",
     "start_time": "2019-05-26T09:32:51.169951Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_binary_positions(df):\n",
    "    zeros = [0 for i in range(df.shape[0])]\n",
    "    PG, SG, F, C = zeros.copy(), zeros.copy(), zeros.copy(), zeros.copy()\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        if \"PG\" in df.loc[i, \"Pos\"]:\n",
    "            PG[i] = 1\n",
    "\n",
    "        elif \"SG\" in df.loc[i, \"Pos\"]:\n",
    "            SG[i] = 1\n",
    "\n",
    "        elif \"C\" in df.loc[i, \"Pos\"]:\n",
    "            C[i] = 1\n",
    "\n",
    "        else:\n",
    "            F[i] = 1\n",
    "\n",
    "    df[\"PG\"] = PG\n",
    "    df[\"SG\"] = SG\n",
    "    df[\"F\"] = F\n",
    "    df[\"C\"] = C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-13T16:06:50.196829Z",
     "start_time": "2018-03-13T16:06:50.193649Z"
    },
    "collapsed": true
   },
   "source": [
    "### Name Standardization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T09:32:53.321615Z",
     "start_time": "2019-05-26T09:32:53.277881Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class NameStandardizer:\n",
    "    # Use the search function on Basketball-Reference.com to generate standard names\n",
    "    def parse_name(self, term, active_years):\n",
    "        search_url = \"https://www.basketball-reference.com/search/search.fcgi?hint=&search={term}&pid=&idx=\"\n",
    "        name_url = search_url.format(term=term.replace(\" \", \"+\"))\n",
    "        soup = BeautifulSoup(urlopen(name_url), \"lxml\")\n",
    "\n",
    "        # Check if there is ambiguity in the name\n",
    "        if soup.find(\"h1\").get_text() != \"Search Results\":\n",
    "            return soup.find(\"h1\").get_text()\n",
    "\n",
    "        elif soup.find(\"div\", id=\"players\", class_=\"current\") == None:\n",
    "            if (len(term.split(\" \")) > 2) or (\".\" in term):\n",
    "                # Parse again without periods and with first two names\n",
    "                new_term = \" \".join(term.replace(\".\", \"\").split(\" \")[:2])\n",
    "                return self.parse_name(new_term, active_years)\n",
    "            else:\n",
    "                return np.nan\n",
    "\n",
    "        else:\n",
    "            items = soup.find(\"div\", id=\"players\", class_=\"current\").find_all(\n",
    "                \"div\", class_=\"search-item-name\"\n",
    "            )\n",
    "            candidates = []\n",
    "\n",
    "            for item in items:\n",
    "                name = item.find(\"a\").get_text()\n",
    "\n",
    "                if \"(\" not in name:\n",
    "                    candidates.append(name)\n",
    "\n",
    "                else:\n",
    "                    career = name[name.find(\"(\") + 1 : name.find(\")\")].split(\"-\")\n",
    "                    if len(career) == 1:\n",
    "                        if int(career[0]) in active_years:\n",
    "                            candidates.append(name[: name.find(\" (\")])\n",
    "                    else:\n",
    "                        start = int(career[0])\n",
    "                        end = int(career[1])\n",
    "\n",
    "                        for year in active_years:\n",
    "                            if year in range(start, end + 1):\n",
    "                                candidates.append(name[: name.find(\" (\")])\n",
    "                                break\n",
    "\n",
    "            if len(candidates) != 0:\n",
    "                for candidate in candidates:\n",
    "                    if term in candidate:\n",
    "                        return candidate\n",
    "                return candidates[0]\n",
    "\n",
    "            else:\n",
    "                return np.nan\n",
    "\n",
    "    def generate_standard_names(self, df, active_years):\n",
    "        names = list(set(df[\"Name\"]))\n",
    "        standard_names = []\n",
    "        errors, confusions = [], []\n",
    "\n",
    "        for i, name in enumerate(tqdm(names)):\n",
    "            standard_name = self.parse_name(name, active_years)\n",
    "\n",
    "            if name != standard_name:\n",
    "                print(\"{} From {} To {}\".format(i, name, standard_name))\n",
    "\n",
    "                if standard_name == np.nan:\n",
    "                    errors.append(name)\n",
    "\n",
    "                elif (\"G-League Stats\" in standard_name) or (\n",
    "                    \"International Stats\" in standard_name\n",
    "                ):\n",
    "                    confusions.append(standard_name)\n",
    "\n",
    "                else:\n",
    "                    standard_names.append(standard_name)\n",
    "            else:\n",
    "                standard_names.append(standard_name)\n",
    "\n",
    "            time.sleep(SECONDS_SLEEP)\n",
    "\n",
    "        return (standard_names, errors, confusions)\n",
    "\n",
    "    def standardize_names(self, df, standard_names, active_years):\n",
    "        names = list(set(df[\"Name\"]))\n",
    "\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "        diff = [name for name in names if name not in standard_names]\n",
    "        print(\"{} names are standardized ...\".format(len(diff)))\n",
    "\n",
    "        names_conversion = {}\n",
    "\n",
    "        for name in tqdm(names):\n",
    "            if name in diff:\n",
    "                names_conversion[name] = self.parse_name(name, active_years)\n",
    "                time.sleep(SECONDS_SLEEP)\n",
    "\n",
    "        for i in range(df.shape[0]):\n",
    "            name = df.loc[i, \"Name\"]\n",
    "            if name in names_conversion.keys():\n",
    "                df.loc[i, \"Name\"] = names_conversion[name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T16:28:09.961055Z",
     "start_time": "2019-05-25T16:28:09.956201Z"
    }
   },
   "source": [
    "### Generate/Load Standardized Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T21:02:12.851421Z",
     "start_time": "2019-05-25T21:02:12.836934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if standard names are already generated\n",
    "if os.path.exists(os.path.join(DATA_DIR, \"Names\", \"standard_names.npy\")):\n",
    "    with open(os.path.join(DATA_DIR, \"Names\", \"standard_names.npy\"), \"rb\") as fp:\n",
    "        standard_names = pickle.load(fp)\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"Names\", \"confusions.npy\"), \"rb\") as fp:\n",
    "        confusions = pickle.load(fp)\n",
    "\n",
    "else:\n",
    "    # Generate standard names for all players from names shown in salary information from RotoGuru\n",
    "    df_salary = utils.csv_concatenate(os.path.join(DATA_DIR, \"DKSalary\"), nested=True)\n",
    "\n",
    "    # Specify current years to avoid duplication across eras (from 2014 to 2019)\n",
    "    active_years = [2014 + i for i in range(6)]\n",
    "\n",
    "    # Takes about 30 mins\n",
    "    standardizer = NameStandardizer()\n",
    "    standard_names, errors, confusions = standardizer.generate_standard_names(\n",
    "        df_salary, active_years\n",
    "    )\n",
    "\n",
    "    # Create a file containing standardized names\n",
    "    with open(os.path.join(DATA_DIR, \"Names\", \"standard_names.npy\"), \"wb\") as fp:\n",
    "        pickle.dump(standard_names, fp)\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"Names\", \"errors.npy\"), \"wb\") as fp:\n",
    "        pickle.dump(errors, fp)\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"Names\", \"confusions.npy\"), \"wb\") as fp:\n",
    "        pickle.dump(confusions, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T21:02:13.820592Z",
     "start_time": "2019-05-25T21:02:13.812961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle edge cases manually as some name searchs return only G-league stats but not NBA records\n",
    "# Cannot be differentiated from players who only played in the G-League at the moment\n",
    "print(confusions)\n",
    "standard_names = standard_names + [\"Derrick Walton\", \"CJ McCollum\", \"Sheldon Mac\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Names and Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T21:13:36.593423Z",
     "start_time": "2019-05-25T21:02:15.734075Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "seasons = [\"2014-15\", \"2015-16\", \"2016-17\", \"2017-18\", \"2018-19\"]\n",
    "active_years = [2014 + i for i in range(6)]\n",
    "\n",
    "standardizer = NameStandardizer()\n",
    "\n",
    "for season in seasons:\n",
    "    print(\"Processing the {} season ...\".format(season))\n",
    "\n",
    "    # Standardize names for salary information\n",
    "    df_salary = utils.csv_concatenate(os.path.join(DATA_DIR, \"DKSalary\", season))\n",
    "    standardizer.standardize_names(df_salary, standard_names, active_years)\n",
    "    fill_pos(df_salary)\n",
    "\n",
    "    # Standardize names for boxscores\n",
    "    df_games = utils.csv_concatenate(os.path.join(DATA_DIR, \"Boxscores\", season))\n",
    "    df_games[\"FPTS\"] = utils.calculate_FPTS(df_games)\n",
    "    add_doubles(df_games)\n",
    "    df_games = df_games.loc[:, DF_VARIABLES]\n",
    "    standardizer.standardize_names(df_games, standard_names, active_years)\n",
    "\n",
    "    # Merge two datasets and save to a csv file\n",
    "    df = pd.merge(\n",
    "        df_salary.drop(\"Team\", axis=1), df_games, on=[\"Name\", \"Date\"], how=\"inner\"\n",
    "    )\n",
    "    df = df[df[\"Pos\"] != 0].sort_values(by=[\"Date\", \"Team\"]).reset_index(drop=True)\n",
    "\n",
    "    # Add \"Value\" variable defined as a ratio between FPTS and Salary\n",
    "    df[\"Value\"] = df[\"FPTS\"] / (df[\"Salary\"] / 1000)\n",
    "    df[\"Value\"] = df[\"Value\"].replace(np.inf, 0).replace(-np.inf, 0)\n",
    "\n",
    "    # Add binary positions for later use in EDA and modelling\n",
    "    add_binary_positions(df)\n",
    "\n",
    "    columns = DF_VARIABLES.copy()\n",
    "\n",
    "    for i, new_column in zip(\n",
    "        [1, 3, 4, 7, 44, 45, 46, 47],\n",
    "        [\"Pos\", \"Salary\", \"Starter\", \"Value\", \"PG\", \"SG\", \"F\", \"C\"],\n",
    "    ):\n",
    "        columns.insert(i, new_column)\n",
    "\n",
    "    df = df.loc[:, columns]\n",
    "    df.to_csv(\n",
    "        os.path.join(DATA_DIR, \"Dataframes\", \"Merged\", \"df_{}.csv\".format(season)),\n",
    "        index=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2e9f85711d675251e4e6c92db989e987b0f16da61e903a803bff5971dd18ff93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
